{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "T.S.Varshini : VIT Chennai (20BRS1125) : varshini.ts2020@vitstudent.ac.in : 9003064047"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a CNN model for Bird species\n",
    "\n",
    "Bird species classification is the process of using machine learning and computer vision techniques to identify and categorize different species of birds based on their visual characteristics. By analyzing images of birds, models can extract features and patterns to accurately classify bird species. This classification is vital for ecological research, wildlife monitoring, and conservation efforts. Advancements in deep learning and the availability of large annotated datasets have improved the accuracy of bird species classification models. Challenges include variations in lighting, pose, and background clutter. Ongoing research focuses on methods like transfer learning and data augmentation to enhance classification performance and contribute to avian biodiversity understanding and conservation.\n",
    "\n",
    "Dataset Link: https://www.kaggle.com/datasets/akash2907/bird-species-classification "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Augmentation\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gen = ImageDataGenerator(rescale=(1./255),horizontal_flip=True,shear_range=0.2)\n",
    "test_gen = ImageDataGenerator(rescale=(1./255))  #--> (0 to 255) convert to (0 to 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 150 images belonging to 16 classes.\n",
      "Found 157 images belonging to 16 classes.\n"
     ]
    }
   ],
   "source": [
    "train = train_gen.flow_from_directory('AI_Assignment3/train_data',\n",
    "                                      target_size=(240, 240),\n",
    "                                      class_mode='categorical', \n",
    "                                      batch_size=8)\n",
    "test = test_gen.flow_from_directory('AI_Assignment3/test_data',\n",
    "                                    target_size=(240, 240),\n",
    "                                      class_mode='categorical', \n",
    "                                      batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'blasti': 0,\n",
       " 'bonegl': 1,\n",
       " 'brhkyt': 2,\n",
       " 'cbrtsh': 3,\n",
       " 'cmnmyn': 4,\n",
       " 'gretit': 5,\n",
       " 'hilpig': 6,\n",
       " 'himbul': 7,\n",
       " 'himgri': 8,\n",
       " 'hsparo': 9,\n",
       " 'indvul': 10,\n",
       " 'jglowl': 11,\n",
       " 'lbicrw': 12,\n",
       " 'mgprob': 13,\n",
       " 'rebimg': 14,\n",
       " 'wcrsrt': 15}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.class_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN\n",
    "\n",
    "from tensorflow.keras.layers import Convolution2D,MaxPooling2D,Flatten,Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import BatchNormalization, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing the seq model\n",
    "model = Sequential()\n",
    "# Adding conv layer with input\n",
    "model.add(Convolution2D(24,(3,3),activation='relu',input_shape=(240, 240, 3)))\n",
    "# Normalizing the conv layer output\n",
    "model.add(BatchNormalization())\n",
    "# Selecting the max values\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "# Dropping the unwanted 20% of data\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Convolution2D(24,(3,3),activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.1))\n",
    "\n",
    "model.add(Convolution2D(36,(3,3),activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Flatten())\n",
    "\n",
    "# Hiddern layers\n",
    "model.add(Dense(62,activation='relu'))\n",
    "model.add(Dense(32,activation='relu'))\n",
    "model.add(Dense(16,activation='relu'))\n",
    "# Output layer\n",
    "model.add(Dense(16,activation='softmax'))\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 238, 238, 24)      672       \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 238, 238, 24)     96        \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 119, 119, 24)     0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 119, 119, 24)      0         \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 117, 117, 24)      5208      \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 117, 117, 24)     96        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 58, 58, 24)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 58, 58, 24)        0         \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 56, 56, 36)        7812      \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 56, 56, 36)       144       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 28, 28, 36)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 28224)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 62)                1749950   \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 32)                2016      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 16)                528       \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 16)                272       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,766,794\n",
      "Trainable params: 1,766,626\n",
      "Non-trainable params: 168\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "19/19 [==============================] - 34s 2s/step - loss: 4.5401 - accuracy: 0.0867 - val_loss: 2.8583 - val_accuracy: 0.0382\n",
      "Epoch 2/50\n",
      "19/19 [==============================] - 32s 2s/step - loss: 3.1770 - accuracy: 0.1867 - val_loss: 3.2686 - val_accuracy: 0.0828\n",
      "Epoch 3/50\n",
      "19/19 [==============================] - 32s 2s/step - loss: 2.8649 - accuracy: 0.1867 - val_loss: 3.5019 - val_accuracy: 0.0764\n",
      "Epoch 4/50\n",
      "19/19 [==============================] - 31s 2s/step - loss: 2.6903 - accuracy: 0.2467 - val_loss: 2.8878 - val_accuracy: 0.0955\n",
      "Epoch 5/50\n",
      "19/19 [==============================] - 33s 2s/step - loss: 2.4688 - accuracy: 0.2600 - val_loss: 3.3933 - val_accuracy: 0.1592\n",
      "Epoch 6/50\n",
      "19/19 [==============================] - 35s 2s/step - loss: 2.4003 - accuracy: 0.3133 - val_loss: 2.8395 - val_accuracy: 0.1083\n",
      "Epoch 7/50\n",
      "19/19 [==============================] - 34s 2s/step - loss: 2.2571 - accuracy: 0.3133 - val_loss: 3.3081 - val_accuracy: 0.0828\n",
      "Epoch 8/50\n",
      "19/19 [==============================] - 33s 2s/step - loss: 2.3001 - accuracy: 0.2867 - val_loss: 2.8571 - val_accuracy: 0.1529\n",
      "Epoch 9/50\n",
      "19/19 [==============================] - 33s 2s/step - loss: 2.1682 - accuracy: 0.3533 - val_loss: 3.1260 - val_accuracy: 0.1529\n",
      "Epoch 10/50\n",
      "19/19 [==============================] - 33s 2s/step - loss: 2.2043 - accuracy: 0.3533 - val_loss: 2.8899 - val_accuracy: 0.0764\n",
      "Epoch 11/50\n",
      "19/19 [==============================] - 33s 2s/step - loss: 2.0786 - accuracy: 0.4200 - val_loss: 2.8782 - val_accuracy: 0.1465\n",
      "Epoch 12/50\n",
      "19/19 [==============================] - 33s 2s/step - loss: 2.0903 - accuracy: 0.4333 - val_loss: 3.0705 - val_accuracy: 0.0892\n",
      "Epoch 13/50\n",
      "19/19 [==============================] - 33s 2s/step - loss: 2.1303 - accuracy: 0.4333 - val_loss: 3.1356 - val_accuracy: 0.0637\n",
      "Epoch 14/50\n",
      "19/19 [==============================] - 33s 2s/step - loss: 2.1588 - accuracy: 0.4067 - val_loss: 4.0664 - val_accuracy: 0.0701\n",
      "Epoch 15/50\n",
      "19/19 [==============================] - 33s 2s/step - loss: 1.8530 - accuracy: 0.4467 - val_loss: 4.0980 - val_accuracy: 0.1146\n",
      "Epoch 16/50\n",
      "19/19 [==============================] - 33s 2s/step - loss: 1.8325 - accuracy: 0.4733 - val_loss: 3.8126 - val_accuracy: 0.1465\n",
      "Epoch 17/50\n",
      "19/19 [==============================] - 32s 2s/step - loss: 1.7795 - accuracy: 0.4333 - val_loss: 3.2398 - val_accuracy: 0.1210\n",
      "Epoch 18/50\n",
      "19/19 [==============================] - 32s 2s/step - loss: 1.6809 - accuracy: 0.4733 - val_loss: 3.2467 - val_accuracy: 0.1975\n",
      "Epoch 19/50\n",
      "19/19 [==============================] - 33s 2s/step - loss: 1.5786 - accuracy: 0.5867 - val_loss: 3.0935 - val_accuracy: 0.1911\n",
      "Epoch 20/50\n",
      "19/19 [==============================] - 33s 2s/step - loss: 1.6826 - accuracy: 0.5533 - val_loss: 4.0868 - val_accuracy: 0.1592\n",
      "Epoch 21/50\n",
      "19/19 [==============================] - 33s 2s/step - loss: 1.4593 - accuracy: 0.5533 - val_loss: 3.7150 - val_accuracy: 0.1019\n",
      "Epoch 22/50\n",
      "19/19 [==============================] - 32s 2s/step - loss: 1.2414 - accuracy: 0.6200 - val_loss: 3.4254 - val_accuracy: 0.1847\n",
      "Epoch 23/50\n",
      "19/19 [==============================] - 33s 2s/step - loss: 1.2498 - accuracy: 0.6333 - val_loss: 4.2261 - val_accuracy: 0.1720\n",
      "Epoch 24/50\n",
      "19/19 [==============================] - 32s 2s/step - loss: 1.4032 - accuracy: 0.6333 - val_loss: 3.2161 - val_accuracy: 0.1847\n",
      "Epoch 25/50\n",
      "19/19 [==============================] - 35s 2s/step - loss: 1.2534 - accuracy: 0.6267 - val_loss: 3.0108 - val_accuracy: 0.2166\n",
      "Epoch 26/50\n",
      "19/19 [==============================] - 37s 2s/step - loss: 1.0374 - accuracy: 0.6733 - val_loss: 2.7891 - val_accuracy: 0.3185\n",
      "Epoch 27/50\n",
      "19/19 [==============================] - 35s 2s/step - loss: 1.0556 - accuracy: 0.6933 - val_loss: 2.7451 - val_accuracy: 0.3376\n",
      "Epoch 28/50\n",
      "19/19 [==============================] - 34s 2s/step - loss: 0.9867 - accuracy: 0.7200 - val_loss: 3.2825 - val_accuracy: 0.2038\n",
      "Epoch 29/50\n",
      "19/19 [==============================] - 33s 2s/step - loss: 1.1565 - accuracy: 0.6533 - val_loss: 3.8446 - val_accuracy: 0.2611\n",
      "Epoch 30/50\n",
      "19/19 [==============================] - 33s 2s/step - loss: 1.1400 - accuracy: 0.6400 - val_loss: 4.6796 - val_accuracy: 0.1529\n",
      "Epoch 31/50\n",
      "19/19 [==============================] - 33s 2s/step - loss: 0.9562 - accuracy: 0.6867 - val_loss: 3.6609 - val_accuracy: 0.2420\n",
      "Epoch 32/50\n",
      "19/19 [==============================] - 33s 2s/step - loss: 0.8197 - accuracy: 0.7600 - val_loss: 3.5550 - val_accuracy: 0.2675\n",
      "Epoch 33/50\n",
      "19/19 [==============================] - 33s 2s/step - loss: 0.8662 - accuracy: 0.7533 - val_loss: 3.0030 - val_accuracy: 0.2675\n",
      "Epoch 34/50\n",
      "19/19 [==============================] - 33s 2s/step - loss: 0.7964 - accuracy: 0.7333 - val_loss: 3.2963 - val_accuracy: 0.3312\n",
      "Epoch 35/50\n",
      "19/19 [==============================] - 33s 2s/step - loss: 0.6865 - accuracy: 0.8000 - val_loss: 3.6021 - val_accuracy: 0.3057\n",
      "Epoch 36/50\n",
      "19/19 [==============================] - 32s 2s/step - loss: 0.7048 - accuracy: 0.7867 - val_loss: 3.7753 - val_accuracy: 0.3185\n",
      "Epoch 37/50\n",
      "19/19 [==============================] - 33s 2s/step - loss: 0.9020 - accuracy: 0.7533 - val_loss: 4.0170 - val_accuracy: 0.2229\n",
      "Epoch 38/50\n",
      "19/19 [==============================] - 33s 2s/step - loss: 0.6139 - accuracy: 0.7733 - val_loss: 4.4188 - val_accuracy: 0.2611\n",
      "Epoch 39/50\n",
      "19/19 [==============================] - 33s 2s/step - loss: 0.6599 - accuracy: 0.7800 - val_loss: 3.9000 - val_accuracy: 0.2803\n",
      "Epoch 40/50\n",
      "19/19 [==============================] - 32s 2s/step - loss: 0.5240 - accuracy: 0.8467 - val_loss: 3.7499 - val_accuracy: 0.3121\n",
      "Epoch 41/50\n",
      "19/19 [==============================] - 32s 2s/step - loss: 0.6306 - accuracy: 0.8467 - val_loss: 4.3729 - val_accuracy: 0.2739\n",
      "Epoch 42/50\n",
      "19/19 [==============================] - 32s 2s/step - loss: 0.6037 - accuracy: 0.8133 - val_loss: 4.2639 - val_accuracy: 0.2866\n",
      "Epoch 43/50\n",
      "19/19 [==============================] - 32s 2s/step - loss: 0.3179 - accuracy: 0.8933 - val_loss: 4.4651 - val_accuracy: 0.2548\n",
      "Epoch 44/50\n",
      "19/19 [==============================] - 32s 2s/step - loss: 0.4534 - accuracy: 0.8533 - val_loss: 4.1723 - val_accuracy: 0.2357\n",
      "Epoch 45/50\n",
      "19/19 [==============================] - 32s 2s/step - loss: 0.4989 - accuracy: 0.8333 - val_loss: 4.0999 - val_accuracy: 0.3057\n",
      "Epoch 46/50\n",
      "19/19 [==============================] - 32s 2s/step - loss: 0.3369 - accuracy: 0.8800 - val_loss: 4.4913 - val_accuracy: 0.3567\n",
      "Epoch 47/50\n",
      "19/19 [==============================] - 32s 2s/step - loss: 0.5257 - accuracy: 0.8400 - val_loss: 5.8113 - val_accuracy: 0.2038\n",
      "Epoch 48/50\n",
      "19/19 [==============================] - 32s 2s/step - loss: 0.3355 - accuracy: 0.9000 - val_loss: 4.8786 - val_accuracy: 0.2611\n",
      "Epoch 49/50\n",
      "19/19 [==============================] - 32s 2s/step - loss: 0.3789 - accuracy: 0.8600 - val_loss: 4.4582 - val_accuracy: 0.1911\n",
      "Epoch 50/50\n",
      "19/19 [==============================] - 33s 2s/step - loss: 0.4143 - accuracy: 0.8733 - val_loss: 4.6135 - val_accuracy: 0.2293\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1fd332b5750>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train,batch_size=8,validation_data=test,epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing\n",
    "\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing import image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 22ms/step\n",
      "8\n",
      "himgri\n"
     ]
    }
   ],
   "source": [
    "# Testing 1 Class - 'himgri'\n",
    "img1 = image.load_img('AI_Assignment3/test_data/himgri/IMG_5384.JPG',target_size=(240,240))\n",
    "img1 = image.img_to_array(img1)\n",
    "img1 = np.expand_dims(img1,axis=0)\n",
    "pred1 = np.argmax(model.predict(img1))\n",
    "print(pred1)\n",
    "img1_output = ['blasti','bonegl','brhkyt','cbrtsh',\n",
    "               'cmnmyn','gretit','hilpig','himbul',\n",
    "               'himgri','hsparo','indvul','jglowl',\n",
    "               'lbicrw','mgprob','rebimg','wcrsrt']\n",
    "print(img1_output[pred1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 23ms/step\n",
      "4\n",
      "cmnmyn\n"
     ]
    }
   ],
   "source": [
    "# Testing 2 Class - 'cmnmyn'\n",
    "img2 = image.load_img('AI_Assignment3/test_data/cmnmyn/DSC_5137.jpg',target_size=(240,240))\n",
    "img2 = image.img_to_array(img2)\n",
    "img2 = np.expand_dims(img2,axis=0)\n",
    "pred2 = np.argmax(model.predict(img2))\n",
    "print(pred2)\n",
    "img2_output = ['blasti','bonegl','brhkyt','cbrtsh',\n",
    "               'cmnmyn','gretit','hilpig','himbul',\n",
    "               'himgri','hsparo','indvul','jglowl',\n",
    "               'lbicrw','mgprob','rebimg','wcrsrt']\n",
    "print(img2_output[pred2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 20ms/step\n",
      "4\n",
      "cmnmyn\n"
     ]
    }
   ],
   "source": [
    "# Testing 3 Class - 'gretit'\n",
    "img3 = image.load_img('AI_Assignment3/test_data/gretit/8537646712_0b282c4c6a_o.jpg',target_size=(240,240))\n",
    "img3 = image.img_to_array(img3)\n",
    "img3 = np.expand_dims(img3,axis=0)\n",
    "pred3 = np.argmax(model.predict(img3))\n",
    "print(pred3)\n",
    "img3_output = ['blasti','bonegl','brhkyt','cbrtsh',\n",
    "               'cmnmyn','gretit','hilpig','himbul',\n",
    "               'himgri','hsparo','indvul','jglowl',\n",
    "               'lbicrw','mgprob','rebimg','wcrsrt']\n",
    "print(img3_output[pred3])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "USING TRANSFER LEARNING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense,Flatten,Input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gen = ImageDataGenerator(rescale=(1./255),horizontal_flip=True,shear_range=0.2)\n",
    "test_gen = ImageDataGenerator(rescale=(1./255))  #--> (0 to 255) convert to (0 to 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 150 images belonging to 16 classes.\n",
      "Found 157 images belonging to 16 classes.\n"
     ]
    }
   ],
   "source": [
    "train = train_gen.flow_from_directory('AI_Assignment3/train_data',\n",
    "                                      target_size=(224, 224),\n",
    "                                      class_mode='categorical', \n",
    "                                      batch_size=8)\n",
    "test = test_gen.flow_from_directory('AI_Assignment3/test_data',\n",
    "                                    target_size=(224, 224),\n",
    "                                      class_mode='categorical', \n",
    "                                      batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'blasti': 0,\n",
       " 'bonegl': 1,\n",
       " 'brhkyt': 2,\n",
       " 'cbrtsh': 3,\n",
       " 'cmnmyn': 4,\n",
       " 'gretit': 5,\n",
       " 'hilpig': 6,\n",
       " 'himbul': 7,\n",
       " 'himgri': 8,\n",
       " 'hsparo': 9,\n",
       " 'indvul': 10,\n",
       " 'jglowl': 11,\n",
       " 'lbicrw': 12,\n",
       " 'mgprob': 13,\n",
       " 'rebimg': 14,\n",
       " 'wcrsrt': 15}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.class_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.vgg16 import VGG16, preprocess_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding the preprocessing layer to the front of vgg\n",
    "\n",
    "vgg = VGG16(include_top=False,weights='imagenet',input_shape=(224,224,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<keras.engine.input_layer.InputLayer object at 0x000001CC86DFA650>\n",
      "<keras.layers.convolutional.conv2d.Conv2D object at 0x000001CC88AD59D0>\n",
      "<keras.layers.convolutional.conv2d.Conv2D object at 0x000001CC8897BD90>\n",
      "<keras.layers.pooling.max_pooling2d.MaxPooling2D object at 0x000001CC88B0E2D0>\n",
      "<keras.layers.convolutional.conv2d.Conv2D object at 0x000001CC88A9C110>\n",
      "<keras.layers.convolutional.conv2d.Conv2D object at 0x000001CC88B5D6D0>\n",
      "<keras.layers.pooling.max_pooling2d.MaxPooling2D object at 0x000001CC88AD6C50>\n",
      "<keras.layers.convolutional.conv2d.Conv2D object at 0x000001CC88B0DBD0>\n",
      "<keras.layers.convolutional.conv2d.Conv2D object at 0x000001CC88B2B890>\n",
      "<keras.layers.convolutional.conv2d.Conv2D object at 0x000001CC88B2BE50>\n",
      "<keras.layers.pooling.max_pooling2d.MaxPooling2D object at 0x000001CC86B45A90>\n",
      "<keras.layers.convolutional.conv2d.Conv2D object at 0x000001CC88B5EBD0>\n",
      "<keras.layers.convolutional.conv2d.Conv2D object at 0x000001CC88AD4D90>\n",
      "<keras.layers.convolutional.conv2d.Conv2D object at 0x000001CC88AEE410>\n",
      "<keras.layers.pooling.max_pooling2d.MaxPooling2D object at 0x000001CC88AD48D0>\n",
      "<keras.layers.convolutional.conv2d.Conv2D object at 0x000001CC86AEDAD0>\n",
      "<keras.layers.convolutional.conv2d.Conv2D object at 0x000001CC88B86F50>\n",
      "<keras.layers.convolutional.conv2d.Conv2D object at 0x000001CC88B6F050>\n",
      "<keras.layers.pooling.max_pooling2d.MaxPooling2D object at 0x000001CCE8EFD710>\n"
     ]
    }
   ],
   "source": [
    "# Train model with existing weights\n",
    "\n",
    "for layer in vgg.layers:\n",
    "  print(layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model with existing weights\n",
    "\n",
    "for layer in vgg.layers:\n",
    "  layer.trainable=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Flatten()(vgg.output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output layer\n",
    "\n",
    "prediction = Dense(16,activation='softmax')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Vgg16 model\n",
    "\n",
    "model1 = Model(inputs=vgg.input,outputs=prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 224, 224, 3)]     0         \n",
      "                                                                 \n",
      " block1_conv1 (Conv2D)       (None, 224, 224, 64)      1792      \n",
      "                                                                 \n",
      " block1_conv2 (Conv2D)       (None, 224, 224, 64)      36928     \n",
      "                                                                 \n",
      " block1_pool (MaxPooling2D)  (None, 112, 112, 64)      0         \n",
      "                                                                 \n",
      " block2_conv1 (Conv2D)       (None, 112, 112, 128)     73856     \n",
      "                                                                 \n",
      " block2_conv2 (Conv2D)       (None, 112, 112, 128)     147584    \n",
      "                                                                 \n",
      " block2_pool (MaxPooling2D)  (None, 56, 56, 128)       0         \n",
      "                                                                 \n",
      " block3_conv1 (Conv2D)       (None, 56, 56, 256)       295168    \n",
      "                                                                 \n",
      " block3_conv2 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_conv3 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_pool (MaxPooling2D)  (None, 28, 28, 256)       0         \n",
      "                                                                 \n",
      " block4_conv1 (Conv2D)       (None, 28, 28, 512)       1180160   \n",
      "                                                                 \n",
      " block4_conv2 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_conv3 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_pool (MaxPooling2D)  (None, 14, 14, 512)       0         \n",
      "                                                                 \n",
      " block5_conv1 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv2 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv3 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_pool (MaxPooling2D)  (None, 7, 7, 512)         0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 25088)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 16)                401424    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 15,116,112\n",
      "Trainable params: 401,424\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\varsh\\AppData\\Local\\Temp\\ipykernel_29248\\3177850299.py:1: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  model1.fit_generator(train,validation_data=test,epochs=4,steps_per_epoch=len(train),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "19/19 [==============================] - 60s 3s/step - loss: 3.6775 - accuracy: 0.2400 - val_loss: 3.8011 - val_accuracy: 0.2930\n",
      "Epoch 2/4\n",
      "19/19 [==============================] - 43s 2s/step - loss: 1.3458 - accuracy: 0.6200 - val_loss: 3.2507 - val_accuracy: 0.3631\n",
      "Epoch 3/4\n",
      "19/19 [==============================] - 44s 2s/step - loss: 0.5048 - accuracy: 0.8200 - val_loss: 3.5071 - val_accuracy: 0.2484\n",
      "Epoch 4/4\n",
      "19/19 [==============================] - 45s 2s/step - loss: 0.2365 - accuracy: 0.9333 - val_loss: 3.4448 - val_accuracy: 0.3503\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1cc88a92dd0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.fit_generator(train,validation_data=test,epochs=4,steps_per_epoch=len(train),\n",
    "                    validation_steps=len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing\n",
    "\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing import image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 95ms/step\n",
      "0\n",
      "blasti\n"
     ]
    }
   ],
   "source": [
    "# Testing 1 Class - 'blasti'\n",
    "img1 = image.load_img('AI_Assignment3/test_data/blasti/DSC_6397.jpg',target_size=(224,224))\n",
    "img1 = image.img_to_array(img1)\n",
    "img1 = np.expand_dims(img1,axis=0)\n",
    "pred1 = np.argmax(model1.predict(img1))\n",
    "print(pred1)\n",
    "img1_output = ['blasti','bonegl','brhkyt','cbrtsh',\n",
    "               'cmnmyn','gretit','hilpig','himbul',\n",
    "               'himgri','hsparo','indvul','jglowl',\n",
    "               'lbicrw','mgprob','rebimg','wcrsrt']\n",
    "print(img1_output[pred1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 100ms/step\n",
      "4\n",
      "cmnmyn\n"
     ]
    }
   ],
   "source": [
    "# Testing 2 Class - 'cmnmyn'\n",
    "img2 = image.load_img('AI_Assignment3/test_data/cmnmyn/DSC_5137.jpg',target_size=(224,224))\n",
    "img2 = image.img_to_array(img2)\n",
    "img2 = np.expand_dims(img2,axis=0)\n",
    "pred2 = np.argmax(model1.predict(img2))\n",
    "print(pred2)\n",
    "img2_output = ['blasti','bonegl','brhkyt','cbrtsh',\n",
    "               'cmnmyn','gretit','hilpig','himbul',\n",
    "               'himgri','hsparo','indvul','jglowl',\n",
    "               'lbicrw','mgprob','rebimg','wcrsrt']\n",
    "print(img2_output[pred2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 92ms/step\n",
      "7\n",
      "himbul\n"
     ]
    }
   ],
   "source": [
    "# Testing 3 Class - 'himbul'\n",
    "img3 = image.load_img('AI_Assignment3/test_data/himbul/6154954471_eefe6e00d1_o.jpg',target_size=(224,224))\n",
    "img3 = image.img_to_array(img3)\n",
    "img3 = np.expand_dims(img3,axis=0)\n",
    "pred3 = np.argmax(model1.predict(img3))\n",
    "print(pred3)\n",
    "img3_output = ['blasti','bonegl','brhkyt','cbrtsh',\n",
    "               'cmnmyn','gretit','hilpig','himbul',\n",
    "               'himgri','hsparo','indvul','jglowl',\n",
    "               'lbicrw','mgprob','rebimg','wcrsrt']\n",
    "print(img3_output[pred3])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
